{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5377174-2fe0-471b-a64e-7877e743cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集地址：https://wukong-dataset.github.io/wukong-dataset/download.html\n",
    "# 论文地址：https://arxiv.org/abs/2202.06767\n",
    "# 数据集百度网盘地址：https://pan.baidu.com/share/init?surl=HbVGfdFvN8FIw7f-lSU4pg 提取码：noah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614ac091-f02a-4bfd-8129-a6b59329c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据下载代码，可在此基础上完善\n",
    "import os\n",
    "import csv\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# 文件和文件夹路径\n",
    "csv_dir = \"wukong_release/\" #元文件存放地址，其格式是以csv的格式存在\n",
    "output_base_dir = \"wukong_train/\" \n",
    "anno_dir = \"wukong_anno/\"\n",
    "log_file = \"/hdownload_errors.log\"  # 新增日志文件\n",
    "\n",
    "# 设置日志记录\n",
    "logging.basicConfig(filename=log_file, level=logging.ERROR)\n",
    "\n",
    "# 最大重试次数\n",
    "max_retry_attempts = 3\n",
    "\n",
    "# 超时限制（秒）\n",
    "timeout = 30\n",
    "\n",
    "# 定义下载函数\n",
    "def download_image(args):\n",
    "    index, row, csv_name = args\n",
    "    image_url = row['url']\n",
    "    caption = row['caption']\n",
    "    filename = f\"{csv_name}_{index + 1}.jpg\"  # 使用索引来命名文件\n",
    "    image_dir = os.path.join(output_base_dir, csv_name)  # 定义 image_dir\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    image_path = os.path.join(image_dir, filename)\n",
    "    retry_attempts = 0\n",
    "    while retry_attempts < max_retry_attempts:\n",
    "        # 使用curl命令下载图像，增加超时限制\n",
    "        result = subprocess.run(['curl', '-o', image_path, '--connect-timeout', str(timeout), image_url], \n",
    "                                stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        if result.returncode == 0:\n",
    "            # 下载成功，退出循环\n",
    "            break\n",
    "        else:\n",
    "            # 下载失败，记录错误信息到日志\n",
    "            error_message = f\"Error downloading image {image_url}: curl returned non-zero exit code {result.returncode}\"\n",
    "            logging.error(error_message)\n",
    "            retry_attempts += 1\n",
    "    # 如果达到最大重试次数仍然失败，记录错误信息\n",
    "    if retry_attempts == max_retry_attempts:\n",
    "        error_message = f\"Failed to download image {image_url} after {max_retry_attempts} attempts.\"\n",
    "        logging.error(error_message)\n",
    "        return\n",
    "    \n",
    "    # 返回文件名和caption\n",
    "    return (filename, caption)\n",
    "\n",
    "# 处理单个CSV文件的下载任务\n",
    "def process_csv(csv_file):\n",
    "    # 创建图像目录\n",
    "    csv_name = os.path.splitext(csv_file)[0]\n",
    "    image_dir = os.path.join(output_base_dir, csv_name)\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "    # 指定要处理的CSV文件\n",
    "    csv_path = os.path.join(csv_dir, csv_file)\n",
    "    image_anno = pd.read_csv(csv_path)\n",
    "\n",
    "    download_results = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=64) as executor:  # 设置线程数量\n",
    "        args = [(index, row, csv_name) for index, row in image_anno.iterrows()]\n",
    "        download_results = list(executor.map(download_image, args))\n",
    "\n",
    "    # 将下载结果转换为DataFrame\n",
    "    download_results = [result for result in download_results if result is not None]\n",
    "    downloaded_data = pd.DataFrame(download_results, columns=['url', 'caption'])\n",
    "\n",
    "    # 只保留文件名和 caption 列\n",
    "    downloaded_data = downloaded_data[['url', 'caption']]\n",
    "\n",
    "    # 将新的DataFrame写回CSV文件中\n",
    "    new_csv_path = os.path.join(anno_dir, f'{csv_name}_updated.csv')\n",
    "    downloaded_data.to_csv(new_csv_path, index=False)\n",
    "\n",
    "# 遍历wukong_release/下的所有CSV文件并下载图像\n",
    "with Pool(processes=16) as pool:  # 设置进程数量\n",
    "    pool.map(process_csv, [csv_file for csv_file in os.listdir(csv_dir) if csv_file.endswith('.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd900ec1-7e24-43e8-a714-9f9675fefc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s5cmd将数据上传至S3数据库\n",
    "# 第一步：安装s5cmd库 \n",
    "# s5cmd库地址 https://github.com/peak/s5cmd\n",
    "!curl -L https://github.com/peak/s5cmd/releases/download/v2.0.0/s5cmd_2.0.0_Linux-64bit.tar.gz | tar -xz s5cmd\n",
    "!chmod +x ./s5cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a170a2c3-6932-4892-aa77-1849dda7f965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 测试s5cmd\n",
    "!./s5cmd ls s3://com.zetyun.data/test1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63ee9f-b561-4dd2-bcd2-f343a31cf831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 第二步，经过反复测试，使用s5cmd上传数据时，如果一次上传太多会导致系统卡死；具体地：当上传较多文件时在/tmp路径下会产生特别多的临时文件导致cpu进程卡死程序终端；\n",
    "# 因此，需要将文件在做切分，代码如下：\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# 源文件和目标文件夹路径\n",
    "source_path = 'wukong_train'\n",
    "SOURCE_DIRS = [source_path + '/' + i for i in os.listdir(source_path)]\n",
    "DEST_DIRS = ['t_' +  i for i in SOURCE_DIRS]\n",
    "\n",
    "# 遍历每个源文件夹和目标文件夹\n",
    "for source_dir, dest_dir in zip(SOURCE_DIRS, DEST_DIRS):\n",
    "    # 创建目标文件夹\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    # 计数器和子文件索引\n",
    "    count = 0\n",
    "    index = 1\n",
    "\n",
    "    # 创建第一个子文件夹\n",
    "    os.makedirs(os.path.join(dest_dir, f\"wukong_{index}\"), exist_ok=True)\n",
    "\n",
    "    # 遍历源文件夹中的图像文件\n",
    "    for file_name in os.listdir(source_dir):\n",
    "        source_file = os.path.join(source_dir, file_name)\n",
    "\n",
    "        # 检查是否达到 4000 张图像\n",
    "        if count == 4000:\n",
    "            # 增加子文件索引\n",
    "            index += 1\n",
    "            # 创建下一个子文件夹\n",
    "            os.makedirs(os.path.join(dest_dir, f\"wukong_{index}\"), exist_ok=True)\n",
    "            # 重置计数器\n",
    "            count = 0\n",
    "\n",
    "        # 将图像文件移动到目标子文件夹\n",
    "        shutil.move(source_file, os.path.join(dest_dir, f\"wukong_{index}\", file_name))\n",
    "\n",
    "        # 增加计数器\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae4805-89de-40ab-a5c7-e4c954368914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第三步，将处理后的文件上传s3\n",
    "import os\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "\n",
    "def process_data(file_path, s3_file):\n",
    "    cmd = f'/home/ec2-user/SageMaker/s5cmd sync {file_path} {s3_file}/'\n",
    "    os.system(cmd)\n",
    "\n",
    "def tanss(local_paths):\n",
    "    files_list = []\n",
    "\n",
    "    for item in os.listdir(local_paths):\n",
    "        local_path = local_paths + '/'+ item + '/'\n",
    "        s3_houzui = local_path.split('/')[1]\n",
    "        s3_file = 's3://rawdata.s3.bucket/wukong/raw_data/wukong_train/' + s3_houzui\n",
    "        files_list.append((local_path, s3_file))\n",
    "        \n",
    "    process_num = 14\n",
    "    pool = mp.Pool(process_num)\n",
    "\n",
    "    for file_info in files_list:\n",
    "        pool.apply_async(process_data, args=file_info)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # 源文件和目标文件夹路径\n",
    "    source_path = 't_wukong_train'\n",
    "    SOURCE_DIRS = [source_path + '/' + i for i in os.listdir(source_path)]\n",
    "    for i in SOURCE_DIRS:\n",
    "        tanss(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a77d4f9-8fc7-4e7b-9d6a-471c4775d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30w图像数据，通过分割之后用s5cmd上传只需要450s左右，而通过下面命令上传需要2500s左右\n",
    "# !./s5cmd sync imgdata/wukong_100m_108  s3://rawdata.s3.bucket/wukong/imgs_wukong/train-imgs/ > /dev/null"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
